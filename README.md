
# ğŸŒ Cloud Chat App with AI Bot ğŸ¤–

A real-time chat application built with **WebSockets**, a **Node.js backend**, and a **local AI-powered chatbot** powered by **Ollama**. This project enables instant messaging between users and a smart assistant that runs completely offline using models like **Mistral** or **LLaMA3**.

---

## ğŸš€ Features

- ğŸ“¡ **Real-time Chat** â€” Seamless communication using WebSockets.
- ğŸ¤– **Local AI Bot** â€” Responses powered by local LLMs (via Ollama), no internet or API keys needed.
- ğŸ’¬ **Minimal UI** â€” Clean and responsive front-end built with HTML & JavaScript.
- ğŸ”’ **Private & Secure** â€” Entirely local; no external API calls.
- âš¡ **Lightweight** â€” No database or heavy dependencies required.

---

## ğŸ—ï¸ Project Structure

```
cloud-chat-app/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ server.js        # Node.js WebSocket server with AI integration
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ client.html      # Minimal chat UI
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸ§  Technologies Used

| Area       | Tools & Libraries               |
|------------|----------------------------------|
| Frontend   | HTML, JavaScript, WebSocket API |
| Backend    | Node.js, WebSocket (`ws`), Axios |
| AI Bot     | Ollama + Local LLMs (`mistral`, `llama3`) |

---

## ğŸ“¦ How It Works

1. **User opens** the `client.html` in a browser.
2. **WebSocket connects** to the Node.js server at `ws://localhost:5050`.
3. **User sends a message**, which is forwarded to the AI bot.
4. **AI response** is generated using a local LLM through Ollama.
5. **Reply is sent back** via WebSocket and displayed on the UI.

---

## ğŸ› ï¸ Setup Instructions

### 1. Install Node.js and Ollama

- [Node.js Download](https://nodejs.org/)
- [Ollama Install](https://ollama.com/download)

### 2. Clone This Repository

```bash
git clone https://github.com/your-username/cloud-chat-app.git
cd cloud-chat-app
```

### 3. Install Dependencies

```bash
cd backend
npm install
```

### 4. Run the Backend Server

```bash
ollama run mistral        # or ollama run llama3
npm start
```

Ensure Ollama is serving on `http://localhost:11434`.

### 5. Open the Frontend

Open `frontend/client.html` in any modern browser.

---

## ğŸ§ª Example Models Supported

- `mistral` (recommended for low memory)
- `llama3` (better quality, needs more RAM)

> ğŸ§  You can switch models by changing the `model` field in `server.js`.

---

## ğŸ’¡ Future Improvements

- [ ] Typing indicator & chat history
- [ ] Emoji support & avatars
- [ ] Dark mode toggle ğŸŒ™
- [ ] Multi-user chat rooms
- [ ] Docker container for deployment

---

## ğŸ“· Preview

![Chat UI Screenshot](chatbot.png) *(optional)*

---

## ğŸ“ License

This project is licensed under the MIT License â€” feel free to use and modify it!

---

## ğŸ™‹â€â™‚ï¸ Contributing

Pull requests are welcome! For major changes, open an issue first to discuss what youâ€™d like to change.
